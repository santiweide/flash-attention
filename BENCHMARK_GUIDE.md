# Flash Attention A100 Benchmark æŒ‡å—

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•å¯¹ `csrc/flash_attn/src/flash_fwd_kernel.h` ä¸­çš„A100å®ç°è¿›è¡Œæ€§èƒ½æµ‹è¯•å’Œåˆ†æã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- NVIDIA GPU (æ¨è A100, ä½† RTX 3090/4090, A6000 ç­‰ä¹Ÿå¯ä»¥)
- GPU è®¡ç®—èƒ½åŠ› >= 8.0 (Ampereæ¶æ„)

### è½¯ä»¶è¦æ±‚
- CUDA >= 12.0
- PyTorch >= 2.2
- Python >= 3.8

## ğŸ”§ å®‰è£…æ­¥éª¤

### 1. å®‰è£…ä¾èµ–
```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install packaging ninja einops

# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

### 2. ç¼–è¯‘å®‰è£… Flash Attention
```bash
cd /Users/michu/Documents/flash-attention

# ä»æºç ç¼–è¯‘å®‰è£… (é¦–æ¬¡å®‰è£…éœ€è¦3-10åˆ†é’Ÿ)
python setup.py install

# æˆ–è€…ä½¿ç”¨pipå®‰è£… (å¦‚æœå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨MAX_JOBSé™åˆ¶å¹¶è¡Œç¼–è¯‘æ•°)
# MAX_JOBS=4 pip install . --no-build-isolation
```

### 3. éªŒè¯å®‰è£…
```bash
python -c "from flash_attn import flash_attn_func; print('âœ… Flash Attention installed successfully!')"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ - è¿è¡ŒBenchmark

### åŸºç¡€ç”¨æ³•
```bash
# è¿è¡Œé»˜è®¤é…ç½® (batch=2, seqlen=1024, heads=8, dim=64)
python minimal_benchmark.py

# è‡ªå®šä¹‰å‚æ•°
python minimal_benchmark.py --batch 4 --seqlen 2048 --nheads 16 --headdim 128

# åªæµ‹è¯•forward pass (ç”¨äºprofiling)
python minimal_benchmark.py --no-grad

# ä½¿ç”¨causal masking
python minimal_benchmark.py --causal

# ä½¿ç”¨bf16æ•°æ®ç±»å‹
python minimal_benchmark.py --dtype bf16

# å¢åŠ é‡å¤æ¬¡æ•°ä»¥è·å¾—æ›´ç¨³å®šçš„ç»“æœ
python minimal_benchmark.py --repeats 100
```

### å¸¸è§é…ç½®ç¤ºä¾‹

#### GPT-2/GPT-3 ç±»ä¼¼é…ç½®
```bash
# GPT-2 Small (12å±‚, 12å¤´, 768ç»´åº¦)
python minimal_benchmark.py --batch 8 --seqlen 1024 --nheads 12 --headdim 64 --causal

# GPT-3 ç±»ä¼¼ (96å±‚, 96å¤´, 12288ç»´åº¦)
python minimal_benchmark.py --batch 2 --seqlen 2048 --nheads 96 --headdim 128 --causal
```

#### BERT ç±»ä¼¼é…ç½®
```bash
# BERT-Base (12å±‚, 12å¤´, 768ç»´åº¦)
python minimal_benchmark.py --batch 16 --seqlen 512 --nheads 12 --headdim 64

# BERT-Large (24å±‚, 16å¤´, 1024ç»´åº¦)
python minimal_benchmark.py --batch 8 --seqlen 512 --nheads 16 --headdim 64
```

#### æ€§èƒ½æµ‹è¯•é…ç½®
```bash
# çŸ­åºåˆ—æµ‹è¯•
python minimal_benchmark.py --batch 32 --seqlen 512 --nheads 8 --headdim 64

# é•¿åºåˆ—æµ‹è¯•
python minimal_benchmark.py --batch 1 --seqlen 8192 --nheads 8 --headdim 64

# æé•¿åºåˆ—æµ‹è¯• (éœ€è¦å¤§æ˜¾å­˜)
python minimal_benchmark.py --batch 1 --seqlen 16384 --nheads 8 --headdim 64
```

## ğŸ“Š æ€§èƒ½ç›‘æµ‹æ–¹æ³•

### 1. åŸºç¡€æ€§èƒ½æŒ‡æ ‡ (å†…ç½®)

è„šæœ¬ä¼šè‡ªåŠ¨è¾“å‡ºä»¥ä¸‹æŒ‡æ ‡ï¼š
- **æ‰§è¡Œæ—¶é—´**: min/avg/max (æ¯«ç§’)
- **ååé‡**: TFLOPs/s (ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—/ç§’)
- **å†…å­˜å ç”¨**: GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µ

è¾“å‡ºç¤ºä¾‹ï¼š
```
Forward Pass Results:
  Time (min):      2.345 ms
  Time (avg):      2.456 ms
  Time (max):      2.678 ms
  TFLOPs/s (peak): 156.32
  TFLOPs/s (avg):  149.87
```

### 2. NVIDIA Nsight Systems (nsys) - ç³»ç»Ÿçº§æ€§èƒ½åˆ†æ

**ç”¨é€”**: åˆ†ææ•´ä½“æ‰§è¡Œæµç¨‹ã€æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆã€æŸ¥çœ‹kernelè°ƒç”¨æ—¶é—´çº¿

```bash
# åŸºç¡€profiling
nsys profile \
    -o flash_attn_profile \
    --stats=true \
    python minimal_benchmark.py --no-grad --repeats 10

# æŸ¥çœ‹æŠ¥å‘Š
nsys stats flash_attn_profile.nsys-rep

# ä½¿ç”¨GUIæŸ¥çœ‹ (éœ€è¦åœ¨æœ¬åœ°ç”µè„‘å®‰è£…Nsight Systems)
# ä¸‹è½½ .nsys-rep æ–‡ä»¶ï¼Œç”¨Nsight Systemsæ‰“å¼€
```

**é«˜çº§é€‰é¡¹**:
```bash
# åŒ…å«CUDA APIå’Œkernelè¯¦ç»†ä¿¡æ¯
nsys profile \
    -o flash_attn_detailed \
    --trace=cuda,nvtx,osrt \
    --stats=true \
    --force-overwrite true \
    python minimal_benchmark.py --batch 4 --seqlen 2048 --no-grad
```

**å…³é”®æŒ‡æ ‡**:
- Kernelæ‰§è¡Œæ—¶é—´å æ¯”
- Memory copyæ—¶é—´
- GPUåˆ©ç”¨ç‡
- SMå ç”¨ç‡

### 3. NVIDIA Nsight Compute (ncu) - Kernelçº§æ€§èƒ½åˆ†æ

**ç”¨é€”**: è¯¦ç»†åˆ†æå•ä¸ªCUDA kernelçš„æ€§èƒ½ï¼Œæ‰¾å‡ºä¼˜åŒ–æœºä¼š

```bash
# åŸºç¡€kernelåˆ†æ
ncu -o flash_attn_kernel \
    --set full \
    python minimal_benchmark.py --no-grad --repeats 1

# åªåˆ†æç‰¹å®škernel (Flash Attentionçš„forward kernel)
ncu -o flash_attn_fwd \
    --set full \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad --repeats 1

# å¿«é€Ÿåˆ†æ (åªçœ‹å…³é”®æŒ‡æ ‡)
ncu --set basic \
    --kernel-name regex:"flash" \
    python minimal_benchmark.py --no-grad --repeats 1
```

**åˆ†æç‰¹å®šæŒ‡æ ‡**:
```bash
# å†…å­˜å¸¦å®½åˆ†æ
ncu --metrics dram__throughput.avg.pct_of_peak_sustained_elapsed \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad

# è®¡ç®—å•å…ƒåˆ©ç”¨ç‡
ncu --metrics sm__throughput.avg.pct_of_peak_sustained_elapsed \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad

# Tensor Coreåˆ©ç”¨ç‡ (FP16/BF16)
ncu --metrics sm__pipe_tensor_cycles_active.avg.pct_of_peak_sustained_elapsed \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad --dtype fp16
```

**æŸ¥çœ‹æŠ¥å‘Š**:
```bash
# ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
ncu --import flash_attn_kernel.ncu-rep

# ä½¿ç”¨GUIæŸ¥çœ‹ (æ›´ç›´è§‚)
# ä¸‹è½½ .ncu-rep æ–‡ä»¶ï¼Œç”¨Nsight Compute GUIæ‰“å¼€
```

### 4. PyTorch Profiler - Pythonçº§æ€§èƒ½åˆ†æ

åˆ›å»º `profile_benchmark.py`:
```python
import torch
from flash_attn import flash_attn_func

# é…ç½®
batch, seqlen, nheads, headdim = 2, 1024, 8, 64
q = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)
k = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)
v = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)

# Warmup
for _ in range(10):
    _ = flash_attn_func(q, k, v)

# Profiling
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    record_shapes=True,
    profile_memory=True,
    with_stack=True,
) as prof:
    for _ in range(10):
        _ = flash_attn_func(q, k, v)

# è¾“å‡ºæŠ¥å‘Š
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=20))

# ä¿å­˜traceæ–‡ä»¶
prof.export_chrome_trace("flash_attn_trace.json")
# ä½¿ç”¨ chrome://tracing æ‰“å¼€
```

è¿è¡Œ:
```bash
python profile_benchmark.py
```

### 5. ä½¿ç”¨ nvidia-smi å®æ—¶ç›‘æ§

åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š
```bash
# å®æ—¶ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
watch -n 0.5 nvidia-smi

# æˆ–è€…ä½¿ç”¨æ›´è¯¦ç»†çš„ç›‘æ§
nvidia-smi dmon -i 0 -s pucvmet -d 1
```

### 6. ä½¿ç”¨ nvtop å¯è§†åŒ–ç›‘æ§

```bash
# å®‰è£… nvtop (å¦‚æœæ²¡æœ‰)
# Ubuntu: apt install nvtop
# å…¶ä»–: https://github.com/Syllo/nvtop

nvtop
```

## ğŸ” æ€§èƒ½æŒ‡æ ‡è§£è¯»

### TFLOPs/s (ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—/ç§’)
- **A100 (40GB/80GB)**: ç†è®ºå³°å€¼ ~312 TFLOPs (FP16/BF16)
- **å®é™…æ€§èƒ½**: Flash Attention é€šå¸¸èƒ½è¾¾åˆ° 150-250 TFLOPs
- **æ›´é«˜æ•°å€¼ = æ›´å¥½çš„æ€§èƒ½**

### Kernelæ‰§è¡Œæ—¶é—´
- å…³æ³¨ **æœ€å°æ—¶é—´** (min time) - ä»£è¡¨æœ€ä½³æƒ…å†µ
- **å¹³å‡æ—¶é—´** (avg time) - ä»£è¡¨å…¸å‹æ€§èƒ½
- å¦‚æœ max å’Œ min å·®å¼‚å¾ˆå¤§ï¼Œå¯èƒ½æœ‰æ€§èƒ½æŠ–åŠ¨

### GPUåˆ©ç”¨ç‡
- **ç›®æ ‡**: >90% GPUåˆ©ç”¨ç‡
- **<80%**: å¯èƒ½æœ‰æ€§èƒ½ç“¶é¢ˆï¼ˆå†…å­˜å¸¦å®½ã€kernel launch overheadç­‰ï¼‰

### å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
- **A100**: ç†è®ºå³°å€¼ ~2TB/s (HBM2e)
- **ç›®æ ‡**: >80% å†…å­˜å¸¦å®½åˆ©ç”¨ç‡
- Flash Attention çš„ä¼˜åŠ¿å°±åœ¨äºé™ä½å†…å­˜è®¿é—®é‡

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### åŸºç¡€æ£€æŸ¥
- [ ] GPUå·¥ä½œé¢‘ç‡æ˜¯å¦è¾¾åˆ°æœ€å¤§ (nvidia-smi æŸ¥çœ‹)
- [ ] æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU
- [ ] CUDAç‰ˆæœ¬æ˜¯å¦æœ€æ–° (å»ºè®® >=12.0)
- [ ] PyTorchç‰ˆæœ¬æ˜¯å¦æœ€æ–° (å»ºè®® >=2.2)

### é…ç½®ä¼˜åŒ–
- [ ] Batch sizeæ˜¯å¦å……åˆ†åˆ©ç”¨GPU
- [ ] ä½¿ç”¨FP16/BF16è€ŒéFP32
- [ ] å¯¹äºdecoderä½¿ç”¨causal mask
- [ ] è€ƒè™‘åºåˆ—é•¿åº¦å¯¹æ€§èƒ½çš„å½±å“

### é«˜çº§ä¼˜åŒ–
- [ ] æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†CUDA Graph (å‡å°‘kernel launchå¼€é”€)
- [ ] æŸ¥çœ‹kernel fusionæƒ…å†µ
- [ ] åˆ†æshared memoryä½¿ç”¨
- [ ] æ£€æŸ¥Tensor Coreåˆ©ç”¨ç‡

## ğŸ“ˆ ä¸æ ‡å‡†PyTorch Attentionå¯¹æ¯”

åˆ›å»ºå¯¹æ¯”è„šæœ¬ `compare_attention.py`:
```python
import torch
import torch.nn.functional as F
from flash_attn import flash_attn_func
import time

def pytorch_attention(q, k, v, causal=False):
    """æ ‡å‡†PyTorch attention"""
    scale = q.shape[-1] ** -0.5
    attn = torch.matmul(q, k.transpose(-2, -1)) * scale
    if causal:
        mask = torch.triu(torch.ones(q.shape[1], k.shape[1]), diagonal=1).bool()
        attn = attn.masked_fill(mask.to(q.device), float('-inf'))
    attn = F.softmax(attn, dim=-1)
    return torch.matmul(attn, v)

batch, seqlen, nheads, headdim = 4, 2048, 8, 64
q = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)
k = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)
v = torch.randn(batch, seqlen, nheads, headdim, device='cuda', dtype=torch.float16)

# Benchmark Flash Attention
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    _ = flash_attn_func(q, k, v)
torch.cuda.synchronize()
flash_time = (time.time() - start) / 100

# Benchmark PyTorch Attention
q_pt = q.transpose(1, 2)  # PyTorch expects [B, H, L, D]
k_pt = k.transpose(1, 2)
v_pt = v.transpose(1, 2)
torch.cuda.synchronize()
start = time.time()
for _ in range(100):
    _ = pytorch_attention(q_pt, k_pt, v_pt)
torch.cuda.synchronize()
pytorch_time = (time.time() - start) / 100

print(f"Flash Attention: {flash_time*1000:.2f} ms")
print(f"PyTorch Attention: {pytorch_time*1000:.2f} ms")
print(f"Speedup: {pytorch_time/flash_time:.2f}x")
print(f"Memory (Flash): {torch.cuda.max_memory_allocated()/1e9:.2f} GB")
```

## ğŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism](https://tridao.me/publications/flash2/flash2.pdf)

### å·¥å…·æ–‡æ¡£
- [NVIDIA Nsight Systems](https://docs.nvidia.com/nsight-systems/)
- [NVIDIA Nsight Compute](https://docs.nvidia.com/nsight-compute/)
- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)

### Cutlassæ–‡æ¡£
- [CUTLASS GitHub](https://github.com/NVIDIA/cutlass)
- [CUTLASS Documentation](https://github.com/NVIDIA/cutlass/tree/main/media/docs)

## ğŸ› å¸¸è§é—®é¢˜

### Q: ç¼–è¯‘å¤±è´¥ "ninja: build stopped: subcommand failed"
**A**: 
```bash
# æ¸…ç†ç¼“å­˜åé‡è¯•
pip uninstall flash-attn -y
rm -rf build/
MAX_JOBS=4 python setup.py install
```

### Q: CUDA out of memory
**A**: å‡å°batch sizeæˆ–sequence length:
```bash
python minimal_benchmark.py --batch 1 --seqlen 1024
```

### Q: æ€§èƒ½ä½äºé¢„æœŸ
**A**: 
1. æ£€æŸ¥GPUé¢‘ç‡: `nvidia-smi -q -d CLOCK`
2. è®¾ç½®GPUä¸ºperformanceæ¨¡å¼: `sudo nvidia-smi -pm 1`
3. ç¡®ä¿ä½¿ç”¨FP16/BF16è€ŒéFP32

### Q: nsys/ncuå‘½ä»¤ä¸å­˜åœ¨
**A**: éœ€è¦å®‰è£…CUDA Toolkit (ä¸åªæ˜¯runtime):
```bash
# æ£€æŸ¥æ˜¯å¦å®‰è£…
which nsys
which ncu

# å¦‚æœæ²¡æœ‰ï¼Œå®‰è£…CUDA Toolkit
# https://developer.nvidia.com/cuda-downloads
```

## ğŸ’¡ é«˜çº§æŠ€å·§

### 1. æ‰¹é‡æµ‹è¯•å¤šä¸ªé…ç½®
```bash
#!/bin/bash
for seqlen in 512 1024 2048 4096; do
    echo "Testing seqlen=$seqlen"
    python minimal_benchmark.py --seqlen $seqlen --no-grad
done
```

### 2. å¯¼å‡ºç»“æœåˆ°CSV
ä¿®æ”¹ `minimal_benchmark.py`ï¼Œæ·»åŠ ï¼š
```python
import csv
with open('results.csv', 'a') as f:
    writer = csv.writer(f)
    writer.writerow([batch, seqlen, nheads, headdim, avg_time, tflops_avg])
```

### 3. è‡ªåŠ¨åŒ–æ€§èƒ½å›å½’æµ‹è¯•
```bash
# baseline
python minimal_benchmark.py > baseline.txt

# ä¿®æ”¹ä»£ç å
python minimal_benchmark.py > current.txt

# å¯¹æ¯”
diff baseline.txt current.txt
```

---

**ç¥ä½ Benchmarké¡ºåˆ©ï¼å¦‚æœ‰é—®é¢˜ï¼Œæ¬¢è¿æŸ¥çœ‹é¡¹ç›®Issuesæˆ–æé—®ã€‚** ğŸš€

