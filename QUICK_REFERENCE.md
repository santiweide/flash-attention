# Flash Attention å¿«é€Ÿå‚è€ƒ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡Œ
```bash
./build_and_run.sh
```

### æ‰‹åŠ¨ç¼–è¯‘
```bash
# ä½¿ç”¨Make
make
./test_flash_attn

# ä½¿ç”¨CMake
mkdir build && cd build
cmake ..
make
./test_flash_attn
```

## ğŸ“ æ ¸å¿ƒç®—æ³• (ä¸€é¡µçº¸)

```python
# Flash Attentionä¼ªä»£ç 
def flash_attention(Q, K, V, block_size):
    M, D = Q.shape
    O = zeros_like(Q)
    l = zeros(M)      # sum of exp
    m = -inf * ones(M) # max
    
    for i in range(0, M, block_size):      # éå†Q blocks
        Q_i = Q[i:i+block_size]
        O_i = zeros_like(Q_i)
        l_i = zeros(block_size)
        m_i = -inf * ones(block_size)
        
        for j in range(0, M, block_size):  # éå†K,V blocks
            K_j = K[j:j+block_size]
            V_j = V[j:j+block_size]
            
            # è®¡ç®—scores
            S_ij = Q_i @ K_j.T * scale
            
            # åœ¨çº¿æ›´æ–°softmax
            m_i_new = max(m_i, rowmax(S_ij))
            P_ij = exp(S_ij - m_i_new)
            l_i_new = exp(m_i - m_i_new) * l_i + rowsum(P_ij)
            
            # æ›´æ–°è¾“å‡º
            correction = exp(m_i - m_i_new) * (l_i / l_i_new)
            O_i = correction * O_i + (P_ij @ V_j) / l_i_new
            
            m_i = m_i_new
            l_i = l_i_new
        
        O[i:i+block_size] = O_i
    
    return O
```

## ğŸ“Š å…³é”®å…¬å¼

### Softmaxåœ¨çº¿æ›´æ–°
```
ç»™å®šï¼š
  m_old, l_old  (æ—§çš„maxå’Œsum)
  S_new         (æ–°çš„scores)

è®¡ç®—ï¼š
  m_new = max(m_old, max(S_new))
  P_new = exp(S_new - m_new)
  l_new = exp(m_old - m_new) * l_old + sum(P_new)
  
  correction = exp(m_old - m_new)
  O_new = correction * O_old + P_new @ V
```

### FLOPsè®¡ç®—
```
Attention(Q,K,V):
  Q@K^T: 2*M*N*D FLOPs
  Softmax: ~5*M*N FLOPs
  P@V: 2*M*N*D FLOPs
  Total: 4*M*N*D FLOPs

å¯¹äºseq_len=S:
  Total = 4*S^2*D*batch*heads
```

### å†…å­˜è®¿é—®
```
æ ‡å‡†Attention: O(S^2) ä¸­é—´å­˜å‚¨
Flash Attention: O(S) ä¸­é—´å­˜å‚¨

èŠ‚çœæ¯”ä¾‹ = S / D (typically 16-128x)
```

## ğŸ”§ é…ç½®å‚æ•°

### `flash_attn_minimal.cu`ä¸­çš„å…³é”®å‚æ•°

```cpp
// å—å¤§å°
constexpr int kBlockM = 64;    // Qçš„å—å¤§å°
constexpr int kBlockN = 64;    // K,Vçš„å—å¤§å°
constexpr int kHeadDim = 64;   // Headç»´åº¦ï¼ˆå›ºå®šï¼‰
constexpr int kNThreads = 128; // æ¯blockçš„çº¿ç¨‹æ•°
```

### è°ƒä¼˜å»ºè®®

| å‚æ•° | æ¨èå€¼ | å½±å“ |
|-----|-------|------|
| kBlockM | 64-128 | è¶Šå¤§è¶Šå¥½ï¼Œä½†å—å…±äº«å†…å­˜é™åˆ¶ |
| kBlockN | 64-128 | åŒä¸Š |
| kNThreads | 128-256 | å–å†³äºè®¡ç®—å¼ºåº¦ |
| GPU Arch | sm_80+ | A100åŠä»¥ä¸Šæ•ˆæœæœ€ä½³ |

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | å¤§å° | è¯´æ˜ |
|-----|------|------|
| `flash_attn_minimal.cu` | ~400è¡Œ | æ ¸å¿ƒkernelå®ç° |
| `test_flash_attn.cu` | ~300è¡Œ | æµ‹è¯•å’Œbenchmark |
| `CMakeLists.txt` | ~100è¡Œ | CMakeé…ç½® |
| `Makefile` | ~60è¡Œ | ç®€å•Makeé…ç½® |
| `build_and_run.sh` | ~100è¡Œ | ä¸€é”®æ„å»ºè„šæœ¬ |
| `README.md` | - | ä¸»æ–‡æ¡£ |
| `ARCHITECTURE.md` | - | æ¶æ„è¯¦è§£ |
| `QUICK_REFERENCE.md` | - | æœ¬æ–‡ä»¶ |

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ç¼–è¯‘é”™è¯¯ "cutlass not found"
```bash
# åˆå§‹åŒ–Cutlasså­æ¨¡å—
cd /path/to/flash-attention
git submodule update --init csrc/cutlass

# æˆ–æ‰‹åŠ¨ä¸‹è½½
git clone https://github.com/NVIDIA/cutlass.git csrc/cutlass
```

### Q2: è¿è¡Œæ—¶é”™è¯¯ "too much shared memory"
```cpp
// å‡å°å—å¤§å°
constexpr int kBlockM = 32;  // ä»64æ”¹ä¸º32
constexpr int kBlockN = 32;
```

### Q3: æ€§èƒ½ä½äºé¢„æœŸ
```bash
# 1. æ£€æŸ¥GPUæ¶æ„
nvidia-smi

# 2. ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„arch
make CUDA_ARCH=-arch=sm_80  # A100
make CUDA_ARCH=-arch=sm_86  # RTX 3090
make CUDA_ARCH=-arch=sm_89  # RTX 4090

# 3. Profileåˆ†æ
ncu --set full ./test_flash_attn
```

### Q4: ç²¾åº¦é—®é¢˜
```
ç›¸å¯¹è¯¯å·® < 5% ä¸ºæ­£å¸¸
åŸå› : FP16ç²¾åº¦é™åˆ¶ + ä¸åŒçš„ç´¯åŠ é¡ºåº
è§£å†³: ä½¿ç”¨æ›´é«˜ç²¾åº¦æˆ–å®ç°Kahanæ±‚å’Œ
```

### Q5: å¦‚ä½•æ·»åŠ causal mask?
```cpp
// åœ¨è®¡ç®—Sä¹‹åæ·»åŠ 
if (q_idx + q_start > k_idx + k_start) {
    shared_mem.S[q_idx * kBlockN + k_idx] = -INFINITY;
}
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### A100 GPU (å…¸å‹å€¼)

| Seq Len | Batch | Heads | Flash (ms) | Ref (ms) | Speedup |
|---------|-------|-------|-----------|----------|---------|
| 128 | 1 | 1 | 0.15 | 0.45 | 3.0x |
| 512 | 1 | 1 | 1.2 | 4.8 | 4.0x |
| 1024 | 1 | 1 | 3.1 | 15.6 | 5.0x |
| 2048 | 1 | 1 | 11.2 | 58.3 | 5.2x |
| 512 | 2 | 8 | 9.5 | 38.2 | 4.0x |

**æ³¨æ„**: è¿™æ˜¯æœ€å°å®ç°çš„æ€§èƒ½ï¼Œå®Œæ•´å®ç°å¿«çº¦1.5-2x

## ğŸ¯ ä¼˜åŒ–æ¸…å•

### å·²å®ç° âœ…
- [x] Tiling (åˆ†å—)
- [x] åœ¨çº¿Softmax
- [x] Kernelèåˆ
- [x] å…±äº«å†…å­˜ä½¿ç”¨
- [x] FP16è®¡ç®—

### æœªå®ç°ï¼ˆå¯æ”¹è¿›ï¼‰âŒ
- [ ] Swizzledå†…å­˜å¸ƒå±€
- [ ] Bank conflictä¼˜åŒ–
- [ ] Warp-level primitives
- [ ] Cutlass GEMMä¼˜åŒ–
- [ ] Causal masking
- [ ] Dropout
- [ ] å‘åä¼ æ’­
- [ ] åŠ¨æ€å—å¤§å°
- [ ] å¤šæ•°æ®ç±»å‹æ”¯æŒ

## ğŸ” Profilingå‘½ä»¤

```bash
# Nsight Systems (ç³»ç»Ÿçº§)
nsys profile -o profile ./test_flash_attn
nsys stats profile.nsys-rep

# Nsight Compute (Kernelçº§)
ncu --set full -o kernel ./test_flash_attn
ncu --import kernel.ncu-rep

# å…³é”®æŒ‡æ ‡
ncu --metrics \
    dram__throughput.avg.pct_of_peak_sustained_elapsed,\
    sm__throughput.avg.pct_of_peak_sustained_elapsed \
    ./test_flash_attn

# GPUä½¿ç”¨ç‡
nvidia-smi dmon -i 0 -s u -d 1
```

## ğŸ“š å­¦ä¹ è·¯å¾„

### Level 1: å…¥é—¨ (1-2å¤©)
1. âœ… é˜…è¯» `README.md`
2. âœ… è¿è¡Œ `./build_and_run.sh`
3. âœ… ç†è§£ç®—æ³•ä¼ªä»£ç 
4. âœ… å¯¹æ¯”è¾“å‡ºç»“æœ

### Level 2: ç†è§£ (3-5å¤©)
1. âœ… é˜…è¯» `ARCHITECTURE.md`
2. âœ… é€è¡Œé˜…è¯» `flash_attn_minimal.cu`
3. âœ… ç†è§£åœ¨çº¿softmaxæ•°å­¦
4. âœ… ç”»å‡ºæ•°æ®æµå›¾

### Level 3: ä¿®æ”¹ (1-2å‘¨)
1. âœ… ä¿®æ”¹å—å¤§å°ï¼Œè§‚å¯Ÿæ€§èƒ½
2. âœ… æ·»åŠ ç®€å•profiling
3. âœ… å®ç°causal mask
4. âœ… æ·»åŠ dropout

### Level 4: ä¼˜åŒ– (1ä¸ªæœˆ+)
1. âœ… å­¦ä¹ Cutlass GEMM
2. âœ… å®ç°swizzling
3. âœ… ä¼˜åŒ–bank conflict
4. âœ… å®ç°å‘åä¼ æ’­
5. âœ… å¯¹æ¯”å®Œæ•´å®ç°

## ğŸ”— ç›¸å…³èµ„æº

### è®ºæ–‡
- [Flash Attention v1](https://arxiv.org/abs/2205.14135) - åŸå§‹è®ºæ–‡
- [Flash Attention v2](https://tridao.me/publications/flash2/flash2.pdf) - æ”¹è¿›ç‰ˆæœ¬
- [Online Normalizer Calculation](https://arxiv.org/abs/1805.02867) - åœ¨çº¿softmaxæ•°å­¦åŸºç¡€

### ä»£ç 
- [Official Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [CUTLASS](https://github.com/NVIDIA/cutlass)
- [xFormers](https://github.com/facebookresearch/xformers)

### æ•™ç¨‹
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [CUTLASS Tutorials](https://github.com/NVIDIA/cutlass/tree/main/examples)
- [Flash Attention Blog](https://tridao.me/blog/2024/flash3/)

### å·¥å…·
- [Nsight Systems](https://developer.nvidia.com/nsight-systems)
- [Nsight Compute](https://developer.nvidia.com/nsight-compute)
- [CUDA Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/)

## ğŸ’¡ æç¤ºå’ŒæŠ€å·§

### è°ƒè¯•
```cpp
// åœ¨kernelä¸­æ‰“å°
if (blockIdx.x == 0 && threadIdx.x == 0) {
    printf("m_shared[0] = %f\n", m_shared[0]);
}

// åŒæ­¥ç‚¹
__syncthreads();  // ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½åˆ°è¾¾è¿™é‡Œ

// æ£€æŸ¥è¾¹ç•Œ
assert(idx < size);
```

### æ€§èƒ½åˆ†æ
```bash
# å¿«é€Ÿæ£€æŸ¥
time ./test_flash_attn

# GPUåˆ©ç”¨ç‡
nvidia-smi --query-gpu=utilization.gpu --format=csv -l 1

# å†…å­˜ä½¿ç”¨
nvidia-smi --query-gpu=memory.used --format=csv -l 1
```

### ç²¾åº¦éªŒè¯
```python
# ç”¨PyTorchéªŒè¯
import torch
import torch.nn.functional as F

Q, K, V = ...  # ä»CUDAå¤åˆ¶
O_ref = F.scaled_dot_product_attention(Q, K, V)
O_flash = ...  # Flash Attentionè¾“å‡º

error = (O_ref - O_flash).abs().max()
print(f"Max error: {error}")
```

## ğŸ“ è´¡çŒ®æŒ‡å—

æƒ³æ”¹è¿›è¿™ä¸ªå®ç°ï¼Ÿ

1. **æŠ¥å‘ŠBug**: åˆ›å»ºIssueæè¿°é—®é¢˜
2. **æ·»åŠ åŠŸèƒ½**: ä¿æŒä»£ç ç®€æ´æ¸…æ™°
3. **æ”¹è¿›æ–‡æ¡£**: æ¬¢è¿æ›´å¥½çš„è§£é‡Š
4. **æ€§èƒ½ä¼˜åŒ–**: è®°å½•å‰åå¯¹æ¯”

ä¿æŒæ•™å­¦ç›®çš„ï¼Œé¿å…è¿‡åº¦å¤æ‚ï¼

---

**å¿«é€Ÿå‚è€ƒå®Œæ¯•ï¼** ğŸ“–

éœ€è¦æ›´å¤šä¿¡æ¯ï¼ŸæŸ¥çœ‹ `README.md` å’Œ `ARCHITECTURE.md`

