# Flash Attention Benchmark å·¥å…·é›†

> å®Œæ•´çš„æ€§èƒ½æµ‹è¯•å’Œåˆ†æå·¥å…·ï¼Œé’ˆå¯¹ `csrc/flash_attn/src/flash_fwd_kernel.h` çš„A100å®ç°

## ğŸ“¦ å·¥å…·æ¦‚è§ˆ

æœ¬å·¥å…·é›†æä¾›äº†ä¸€å¥—å®Œæ•´çš„benchmarkå’Œprofilingæ–¹æ¡ˆï¼š

| å·¥å…· | ç”¨é€” | éš¾åº¦ |
|-----|------|------|
| `minimal_benchmark.py` | å¿«é€Ÿæ€§èƒ½æµ‹è¯• | â­ï¸ ç®€å• |
| `compare_implementations.py` | å¯¹æ¯”ä¸åŒå®ç° | â­ï¸ ç®€å• |
| `auto_profile.sh` | ä¸€é”®å…¨é¢åˆ†æ | â­ï¸â­ï¸ ä¸­ç­‰ |
| `QUICK_START_BENCHMARK.md` | å¿«é€Ÿå…¥é—¨æŒ‡å— | ğŸ“– æ–‡æ¡£ |
| `BENCHMARK_GUIDE.md` | è¯¦ç»†ä½¿ç”¨æ‰‹å†Œ | ğŸ“– æ–‡æ¡£ |

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: å®‰è£…
```bash
cd /Users/michu/Documents/flash-attention
pip install ninja packaging einops
python setup.py install
```

### æ­¥éª¤2: åŸºç¡€æµ‹è¯•
```bash
# è¿è¡Œæœ€ç®€å•çš„benchmark
python minimal_benchmark.py
```

é¢„æœŸè¾“å‡º:
```
================================================================================
Flash Attention Benchmark
================================================================================
GPU: NVIDIA A100-SXM4-40GB
Compute Capability: 8.0
...
Forward Pass Results:
  Time (min):      2.345 ms
  TFLOPs/s (peak): 156.32
âœ… Benchmark completed!
```

### æ­¥éª¤3: å¯¹æ¯”æµ‹è¯• (å¯é€‰)
```bash
# å¯¹æ¯”Flash Attention vs PyTorch
python compare_implementations.py
```

é¢„æœŸè¾“å‡º:
```
Implementation            Time (ms)       Speedup
--------------------------------------------------------------------------------
Flash Attention              2.345         1.00x
PyTorch Standard            15.678         0.15x
ğŸ† Fastest: Flash Attention (2.345 ms)
```

## ğŸ“‹ è¯¦ç»†å·¥å…·è¯´æ˜

### 1. minimal_benchmark.py - æ ¸å¿ƒBenchmarkå·¥å…·

**åŠŸèƒ½**:
- âœ… æµ‹é‡Forward/Backwardæ‰§è¡Œæ—¶é—´
- âœ… è®¡ç®—TFLOPs/sååé‡
- âœ… ç›‘æ§GPUå†…å­˜ä½¿ç”¨
- âœ… æ”¯æŒå¤šç§é…ç½®å‚æ•°

**åŸºç¡€ç”¨æ³•**:
```bash
# é»˜è®¤é…ç½®
python minimal_benchmark.py

# è‡ªå®šä¹‰æ‰€æœ‰å‚æ•°
python minimal_benchmark.py \
    --batch 4 \
    --seqlen 2048 \
    --nheads 16 \
    --headdim 128 \
    --causal \
    --dtype fp16 \
    --repeats 30
```

**å‚æ•°è¯´æ˜**:
- `--batch`: Batch size (é»˜è®¤: 2)
- `--seqlen`: åºåˆ—é•¿åº¦ (é»˜è®¤: 1024)
- `--nheads`: Headæ•°é‡ (é»˜è®¤: 8)
- `--headdim`: Headç»´åº¦ (é»˜è®¤: 64)
- `--causal`: ä½¿ç”¨causal masking (ç”¨äºdecoder)
- `--dtype`: æ•°æ®ç±»å‹ fp16/bf16 (é»˜è®¤: fp16)
- `--repeats`: é‡å¤æ¬¡æ•° (é»˜è®¤: 30)
- `--no-grad`: åªæµ‹forward pass (ç”¨äºprofiling)

**å¸¸è§åœºæ™¯**:
```bash
# GPT-2 é…ç½®
python minimal_benchmark.py --batch 8 --seqlen 1024 --nheads 12 --headdim 64 --causal

# BERTé…ç½®
python minimal_benchmark.py --batch 16 --seqlen 512 --nheads 12 --headdim 64

# é•¿åºåˆ—æµ‹è¯•
python minimal_benchmark.py --batch 1 --seqlen 8192 --nheads 8 --headdim 64
```

**è¾“å‡ºæŒ‡æ ‡**:
- æ‰§è¡Œæ—¶é—´ (min/avg/max)
- ååé‡ (TFLOPs/s)
- å†…å­˜å ç”¨ (GB)

---

### 2. compare_implementations.py - å®ç°å¯¹æ¯”å·¥å…·

**åŠŸèƒ½**:
- âœ… å¯¹æ¯”Flash Attention vs PyTorch
- âœ… å¯¹æ¯”Flash Attention vs xFormers
- âœ… å¯¹æ¯”Flash Attention vs PyTorch SDPA
- âœ… æ˜¾ç¤ºåŠ é€Ÿæ¯”å’Œå†…å­˜èŠ‚çœ

**ç”¨æ³•**:
```bash
# åŸºç¡€å¯¹æ¯”
python compare_implementations.py

# è‡ªå®šä¹‰é…ç½®
python compare_implementations.py \
    --batch 4 \
    --seqlen 2048 \
    --nheads 16 \
    --headdim 128 \
    --causal
```

**è¾“å‡ºç¤ºä¾‹**:
```
================================================================================
Attention Implementation Comparison
================================================================================
GPU: NVIDIA A100-SXM4-40GB
Config: batch=4, seqlen=2048, nheads=16, headdim=128, causal=False
================================================================================

Results:
================================================================================
Implementation            Time (ms)       Memory (MB)     Speedup
--------------------------------------------------------------------------------
Flash Attention              4.567          1024.5         1.00x
PyTorch Standard            23.456          4096.8         0.19x
PyTorch SDPA                 6.789          1536.2         0.67x
xFormers                     5.234          1152.3         0.87x
================================================================================

ğŸ† Fastest: Flash Attention (4.567 ms)
ğŸ’¾ Lowest Memory: Flash Attention (1024.5 MB)
```

**å…³é”®æŒ‡æ ‡**:
- **Speedup**: Flash Attentionç›¸å¯¹äºå…¶ä»–å®ç°çš„åŠ é€Ÿæ¯”
- **Memory**: å†…å­˜ä½¿ç”¨å¯¹æ¯”
- **Time**: ç»å¯¹æ‰§è¡Œæ—¶é—´

---

### 3. auto_profile.sh - è‡ªåŠ¨åŒ–Profilingè„šæœ¬

**åŠŸèƒ½**:
- âœ… ä¸€é”®è¿è¡Œæ‰€æœ‰profilingå·¥å…·
- âœ… ç”ŸæˆNsight SystemsæŠ¥å‘Š
- âœ… ç”ŸæˆNsight ComputeæŠ¥å‘Š
- âœ… æ”¶é›†GPUä¿¡æ¯
- âœ… è¿è¡Œå®ç°å¯¹æ¯”
- âœ… ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š

**ç”¨æ³•**:
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
./auto_profile.sh

# è‡ªå®šä¹‰é…ç½®
BATCH=4 SEQLEN=2048 NHEADS=16 ./auto_profile.sh

# æŒ‡å®šè¾“å‡ºç›®å½•
OUTPUT_DIR=my_results ./auto_profile.sh
```

**ç¯å¢ƒå˜é‡**:
- `BATCH`: Batch size (é»˜è®¤: 2)
- `SEQLEN`: åºåˆ—é•¿åº¦ (é»˜è®¤: 1024)
- `NHEADS`: Headæ•°é‡ (é»˜è®¤: 8)
- `HEADDIM`: Headç»´åº¦ (é»˜è®¤: 64)
- `REPEATS`: é‡å¤æ¬¡æ•° (é»˜è®¤: 10)
- `OUTPUT_DIR`: è¾“å‡ºç›®å½• (é»˜è®¤: profile_results)

**ç”Ÿæˆçš„æ–‡ä»¶**:
```
profile_results/
â”œâ”€â”€ ğŸ“„ SUMMARY.md                      â† ä»è¿™é‡Œå¼€å§‹ï¼
â”œâ”€â”€ baseline_benchmark.txt            # åŸºç¡€æ€§èƒ½æ•°æ®
â”œâ”€â”€ comparison_results.txt            # å®ç°å¯¹æ¯”
â”œâ”€â”€ gpu_info.txt                      # GPUä¿¡æ¯
â”œâ”€â”€ gpu_info_detailed.txt             # è¯¦ç»†GPUä¿¡æ¯
â”‚
â”œâ”€â”€ ğŸ” Nsight Systems (ç³»ç»Ÿçº§åˆ†æ)
â”‚   â”œâ”€â”€ flash_attn_nsys.nsys-rep     # GUIæ‰“å¼€ â† æ¨è
â”‚   â”œâ”€â”€ nsys_stats.txt                # æ–‡æœ¬ç»Ÿè®¡
â”‚   â””â”€â”€ nsys_output.txt               # æ§åˆ¶å°è¾“å‡º
â”‚
â””â”€â”€ ğŸ” Nsight Compute (Kernelçº§åˆ†æ)
    â”œâ”€â”€ flash_attn_ncu_basic.ncu-rep  # å¿«é€Ÿåˆ†æ
    â”œâ”€â”€ flash_attn_ncu_full.ncu-rep   # è¯¦ç»†åˆ†æ â† æ¨è
    â”œâ”€â”€ ncu_memory_metrics.txt        # å†…å­˜å¸¦å®½
    â””â”€â”€ ncu_compute_metrics.txt       # è®¡ç®—åˆ©ç”¨ç‡
```

**æŸ¥çœ‹ç»“æœ**:
```bash
# 1. æŸ¥çœ‹æ±‡æ€»
cat profile_results/SUMMARY.md

# 2. ä½¿ç”¨GUIæŸ¥çœ‹ (æ›´ç›´è§‚)
# - ä¸‹è½½ .nsys-rep æ–‡ä»¶ï¼Œç”¨ Nsight Systems æ‰“å¼€
# - ä¸‹è½½ .ncu-rep æ–‡ä»¶ï¼Œç”¨ Nsight Compute æ‰“å¼€
```

---

## ğŸ” æ€§èƒ½åˆ†ææ–¹æ³•

### æ–¹æ³•1: å¿«é€Ÿæµ‹è¯• (1åˆ†é’Ÿ)
```bash
python minimal_benchmark.py
```
**é€‚ç”¨äº**: å¿«é€ŸéªŒè¯æ€§èƒ½ã€å¯¹æ¯”ä¸åŒé…ç½®

### æ–¹æ³•2: å¯¹æ¯”æµ‹è¯• (2åˆ†é’Ÿ)
```bash
python compare_implementations.py
```
**é€‚ç”¨äº**: éªŒè¯Flash Attentionçš„åŠ é€Ÿæ•ˆæœ

### æ–¹æ³•3: å®Œæ•´Profiling (10-30åˆ†é’Ÿ)
```bash
./auto_profile.sh
```
**é€‚ç”¨äº**: æ·±å…¥æ€§èƒ½åˆ†æã€æ‰¾å‡ºç“¶é¢ˆã€ä¼˜åŒ–kernel

### æ–¹æ³•4: æ‰‹åŠ¨Profiling (é«˜çº§)

**Nsight Systems** (ç³»ç»Ÿçº§):
```bash
nsys profile -o profile --stats=true \
    python minimal_benchmark.py --no-grad
```

**Nsight Compute** (Kernelçº§):
```bash
ncu --set full -o kernel_profile \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad
```

**å®æ—¶ç›‘æ§**:
```bash
# ç»ˆç«¯1: è¿è¡Œbenchmark
python minimal_benchmark.py --repeats 1000

# ç»ˆç«¯2: ç›‘æ§GPU
watch -n 0.5 nvidia-smi
# æˆ–
nvtop
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡è§£è¯»

### TFLOPs/s (ååé‡)

| GPU | ç†è®ºå³°å€¼ (FP16) | Flash Attn å…¸å‹å€¼ | ç›®æ ‡åˆ©ç”¨ç‡ |
|-----|----------------|-------------------|-----------|
| A100 | ~312 TFLOPs | 150-250 TFLOPs | >50% |
| A6000 | ~154 TFLOPs | 80-130 TFLOPs | >50% |
| RTX 4090 | ~330 TFLOPs | 150-280 TFLOPs | >50% |
| RTX 3090 | ~142 TFLOPs | 70-120 TFLOPs | >50% |

**è§£è¯»**:
- âœ… **å¥½**: TFLOPs/s > ç†è®ºå³°å€¼çš„50%
- âš ï¸ **ä¸€èˆ¬**: 30-50%
- âŒ **å·®**: <30% (éœ€è¦ä¼˜åŒ–)

### æ‰§è¡Œæ—¶é—´å‚è€ƒ (A100, FP16)

| Batch | SeqLen | Heads | HeadDim | å…¸å‹æ—¶é—´ |
|-------|--------|-------|---------|---------|
| 1 | 512 | 8 | 64 | ~0.5 ms |
| 2 | 1024 | 8 | 64 | ~2 ms |
| 4 | 2048 | 8 | 64 | ~8 ms |
| 8 | 4096 | 8 | 64 | ~30 ms |
| 1 | 8192 | 8 | 64 | ~15 ms |
| 1 | 16384 | 8 | 64 | ~60 ms |

### åŠ é€Ÿæ¯”

| å¯¹æ¯”å®ç° | å…¸å‹åŠ é€Ÿæ¯” | å¤‡æ³¨ |
|---------|-----------|------|
| PyTorch Standard | 3-8x | åºåˆ—è¶Šé•¿åŠ é€Ÿè¶Šæ˜æ˜¾ |
| PyTorch SDPA | 1.2-2x | PyTorch 2.0+ çš„ä¼˜åŒ–å®ç° |
| xFormers | 0.9-1.5x | å¦ä¸€ä¸ªé«˜æ•ˆå®ç° |

### GPUåˆ©ç”¨ç‡

**æŸ¥çœ‹æ–¹æ³•**:
```bash
nvidia-smi dmon -i 0 -s u -d 1
```

**ç›®æ ‡**:
- âœ… **å¥½**: >90%
- âš ï¸ **ä¸€èˆ¬**: 70-90%
- âŒ **å·®**: <70% (æœ‰ç“¶é¢ˆ)

---

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: å¼€å‘è°ƒè¯•
```bash
# å¿«é€ŸéªŒè¯ä¿®æ”¹æ˜¯å¦æ­£ç¡®
python minimal_benchmark.py --seqlen 512 --repeats 5
```

### åœºæ™¯2: æ€§èƒ½å›å½’æµ‹è¯•
```bash
# ä¿å­˜baseline
python minimal_benchmark.py > baseline.txt

# ä¿®æ”¹ä»£ç å
python minimal_benchmark.py > current.txt

# å¯¹æ¯”
diff baseline.txt current.txt
```

### åœºæ™¯3: å¯»æ‰¾æœ€ä½³é…ç½®
```bash
#!/bin/bash
for bs in 1 2 4 8; do
  for sl in 512 1024 2048; do
    echo "batch=$bs, seqlen=$sl"
    python minimal_benchmark.py --batch $bs --seqlen $sl --no-grad
  done
done
```

### åœºæ™¯4: æ·±å…¥æ€§èƒ½åˆ†æ
```bash
# ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬
./auto_profile.sh

# æŸ¥çœ‹æŠ¥å‘Š
cat profile_results/SUMMARY.md

# ç”¨GUIåˆ†ækernel
# æ‰“å¼€ profile_results/flash_attn_ncu_full.ncu-rep
```

### åœºæ™¯5: è®ºæ–‡/æŠ¥å‘Š
```bash
# å¯¹æ¯”å¤šä¸ªå®ç°
python compare_implementations.py --batch 4 --seqlen 2048 > paper_results.txt

# å¤šä¸ªé…ç½®æµ‹è¯•
for seqlen in 512 1024 2048 4096 8192; do
    python minimal_benchmark.py --seqlen $seqlen
done | tee scaling_results.txt
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: ImportError: cannot import flash_attn
```bash
# é‡æ–°å®‰è£…
pip uninstall flash-attn -y
python setup.py install
```

### Q2: CUDA out of memory
```bash
# å‡å°é…ç½®
python minimal_benchmark.py --batch 1 --seqlen 1024
```

### Q3: ç¼–è¯‘æ—¶é—´å¤ªé•¿
```bash
# é™åˆ¶å¹¶è¡Œç¼–è¯‘æ•°
MAX_JOBS=4 python setup.py install
```

### Q4: æ‰¾ä¸åˆ°nsys/ncuå‘½ä»¤
```bash
# éœ€è¦å®‰è£…å®Œæ•´CUDA Toolkit
# https://developer.nvidia.com/cuda-downloads

# æˆ–æ·»åŠ åˆ°PATH
export PATH=/usr/local/cuda/bin:$PATH
```

### Q5: æ€§èƒ½è¿œä½äºé¢„æœŸ
```bash
# æ£€æŸ¥GPUçŠ¶æ€
nvidia-smi

# è®¾ç½®æ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1

# æ£€æŸ¥é¢‘ç‡
nvidia-smi -q -d CLOCK | grep Graphics

# æ£€æŸ¥åå°è¿›ç¨‹
ps aux | grep python
```

---

## ğŸ“š å­¦ä¹ èµ„æº

### å¿…è¯»æ–‡æ¡£
1. **å¿«é€Ÿå…¥é—¨**: `QUICK_START_BENCHMARK.md` â† ä»è¿™é‡Œå¼€å§‹
2. **è¯¦ç»†æŒ‡å—**: `BENCHMARK_GUIDE.md`
3. **æœ¬æ–‡æ¡£**: `README_BENCHMARK.md`

### Flash Attentionè®ºæ–‡
- [Flash Attention v1](https://arxiv.org/abs/2205.14135) - åŸå§‹è®ºæ–‡
- [Flash Attention v2](https://tridao.me/publications/flash2/flash2.pdf) - ä¼˜åŒ–ç‰ˆæœ¬
- [Flash Attention v3](https://tridao.me/publications/flash3/flash3.pdf) - Hopperæ¶æ„

### æ ¸å¿ƒå®ç°æ–‡ä»¶
- `csrc/flash_attn/src/flash_fwd_kernel.h` - **A100 forward kernel**
- `csrc/flash_attn/src/flash_bwd_kernel.h` - **A100 backward kernel**
- `hopper/flash_fwd_kernel_sm90.h` - H100ä¼˜åŒ–ç‰ˆæœ¬
- `hopper/mainloop_fwd_sm90_tma_gmma_ws.hpp` - Hopperä¸»å¾ªç¯

### Cutlassèµ„æº
- [Cutlass GitHub](https://github.com/NVIDIA/cutlass)
- [Cutlass æ–‡æ¡£](https://github.com/NVIDIA/cutlass/tree/main/media/docs)
- [Cutlass Examples](https://github.com/NVIDIA/cutlass/tree/main/examples)

### NVIDIAå·¥å…·
- [Nsight Systems æ–‡æ¡£](https://docs.nvidia.com/nsight-systems/)
- [Nsight Compute æ–‡æ¡£](https://docs.nvidia.com/nsight-compute/)
- [CUDA Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

---

## ğŸ’¡ æœ€ä½³å®è·µ

### Benchmarkæœ€ä½³å®è·µ
1. âœ… **é¢„çƒ­å……åˆ†**: warmupè‡³å°‘5æ¬¡
2. âœ… **é‡å¤å¤šæ¬¡**: repeatsè‡³å°‘30æ¬¡
3. âœ… **æ§åˆ¶å˜é‡**: æ¯æ¬¡åªæ”¹å˜ä¸€ä¸ªå‚æ•°
4. âœ… **è®°å½•ç¯å¢ƒ**: GPUå‹å·ã€CUDAç‰ˆæœ¬ã€PyTorchç‰ˆæœ¬
5. âœ… **å¯¹æ¯”baseline**: æ€»æ˜¯å’Œå·²çŸ¥ç»“æœå¯¹æ¯”

### Profilingæœ€ä½³å®è·µ
1. âœ… **ç”¨--no-grad**: profilingæ—¶åªæµ‹forward pass
2. âœ… **å‡å°‘repeats**: profilingæ—¶ç”¨å°‘é‡é‡å¤
3. âœ… **å…ˆç”¨nsys**: ç³»ç»Ÿçº§åˆ†ææ‰¾å¤§æ–¹å‘
4. âœ… **å†ç”¨ncu**: kernelçº§åˆ†ææ‰¾ç»†èŠ‚
5. âœ… **ç”¨GUIæŸ¥çœ‹**: æ¯”æ–‡æœ¬æŠ¥å‘Šæ›´ç›´è§‚

### æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•
- [ ] GPUå¤„äºæœ€é«˜æ€§èƒ½æ¨¡å¼
- [ ] æ²¡æœ‰å…¶ä»–è¿›ç¨‹å ç”¨GPU
- [ ] ä½¿ç”¨FP16/BF16è€ŒéFP32
- [ ] Batch sizeå……åˆ†åˆ©ç”¨GPU
- [ ] CUDAå’ŒPyTorchç‰ˆæœ¬æœ€æ–°
- [ ] GPUæ¸©åº¦æ­£å¸¸ (<85Â°C)

---

## ğŸ¤ è´¡çŒ®

å‘ç°é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Ÿ

1. æäº¤Issueåˆ° [Flash Attention GitHub](https://github.com/Dao-AILab/flash-attention/issues)
2. æˆ–è€…ä¿®æ”¹è¿™äº›benchmarkè„šæœ¬å¹¶åˆ†äº«

---

## ğŸ“„ License

è¿™äº›benchmarkå·¥å…·éµå¾ªFlash Attentionçš„LICENSE (BSD-3-Clause)

---

## ğŸ‰ æ€»ç»“

ä½ ç°åœ¨æ‹¥æœ‰äº†ä¸€å¥—å®Œæ•´çš„Flash Attention benchmarkå·¥å…·ï¼

**æœ€ç®€å•çš„å¼€å§‹æ–¹å¼**:
```bash
# 1. å®‰è£…
python setup.py install

# 2. æµ‹è¯•
python minimal_benchmark.py

# 3. å¯¹æ¯”
python compare_implementations.py

# 4. æ·±å…¥åˆ†æ (å¯é€‰)
./auto_profile.sh
```

**ä¸‹ä¸€æ­¥**:
- ğŸ“– é˜…è¯» `QUICK_START_BENCHMARK.md` äº†è§£è¯¦ç»†ç”¨æ³•
- ğŸ”¬ è¿è¡Œ `auto_profile.sh` è·å¾—å®Œæ•´æ€§èƒ½æŠ¥å‘Š
- ğŸ“Š é˜…è¯» `BENCHMARK_GUIDE.md` å­¦ä¹ é«˜çº§æŠ€å·§

---

**ç¥ä½ Benchmarké¡ºåˆ©ï¼** ğŸš€

æœ‰é—®é¢˜éšæ—¶æŸ¥çœ‹æ–‡æ¡£æˆ–æIssueï¼

