# å¿«é€Ÿå¼€å§‹ - Flash Attention Benchmark

> é’ˆå¯¹ `csrc/flash_attn/src/flash_fwd_kernel.h` (A100å®ç°) çš„æœ€å°åŒ–benchmarkæ–¹æ¡ˆ

## ğŸš€ ä¸‰æ­¥å¿«é€Ÿå¼€å§‹

### 1ï¸âƒ£ å®‰è£…ä¾èµ–
```bash
# å®‰è£…Flash Attention
cd /Users/michu/Documents/flash-attention
pip install ninja packaging einops
python setup.py install
```

### 2ï¸âƒ£ è¿è¡ŒåŸºç¡€Benchmark
```bash
# é»˜è®¤é…ç½® (batch=2, seqlen=1024, heads=8, dim=64)
python minimal_benchmark.py

# è‡ªå®šä¹‰é…ç½®
python minimal_benchmark.py --batch 4 --seqlen 2048 --nheads 16 --headdim 128
```

### 3ï¸âƒ£ å¯¹æ¯”ä¸åŒå®ç°
```bash
python compare_implementations.py --batch 4 --seqlen 2048
```

## ğŸ“Š å·¥å…·è¯´æ˜

### å·¥å…·1: minimal_benchmark.py
**ç”¨é€”**: å¿«é€Ÿæµ‹è¯•Flash Attentionæ€§èƒ½

**æ‰§è¡Œå‘½ä»¤**:
```bash
# åŸºç¡€æµ‹è¯•
python minimal_benchmark.py

# å®Œæ•´å‚æ•°ç¤ºä¾‹
python minimal_benchmark.py \
    --batch 4 \
    --seqlen 2048 \
    --nheads 8 \
    --headdim 64 \
    --causal \
    --dtype fp16 \
    --repeats 30
```

**è¾“å‡ºæŒ‡æ ‡**:
- â±ï¸ **æ‰§è¡Œæ—¶é—´**: min/avg/max (æ¯«ç§’)
- ğŸš€ **ååé‡**: TFLOPs/s (ä¸‡äº¿æ¬¡æµ®ç‚¹è¿ç®—/ç§’)
- ğŸ’¾ **å†…å­˜å ç”¨**: GPUæ˜¾å­˜ä½¿ç”¨

**ç¤ºä¾‹è¾“å‡º**:
```
Forward Pass Results:
  Time (min):      2.345 ms
  Time (avg):      2.456 ms
  TFLOPs/s (peak): 156.32 TFLOPs/s
  TFLOPs/s (avg):  149.87 TFLOPs/s
```

### å·¥å…·2: compare_implementations.py
**ç”¨é€”**: å¯¹æ¯”Flash Attention vs PyTorchç­‰å®ç°

**æ‰§è¡Œå‘½ä»¤**:
```bash
python compare_implementations.py --batch 4 --seqlen 2048
```

**è¾“å‡ºç¤ºä¾‹**:
```
Implementation            Time (ms)       Memory (MB)     Speedup
--------------------------------------------------------------------------------
Flash Attention              2.345          512.1         1.00x
PyTorch Standard            15.678         2048.3         0.15x
PyTorch SDPA                 3.456          768.2         0.68x
xFormers                     2.890          624.5         0.81x

ğŸ† Fastest: Flash Attention (2.345 ms)
ğŸ’¾ Lowest Memory: Flash Attention (512.1 MB)
```

### å·¥å…·3: auto_profile.sh
**ç”¨é€”**: ä¸€é”®è¿è¡Œæ‰€æœ‰profilingå·¥å…·

**æ‰§è¡Œå‘½ä»¤**:
```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
./auto_profile.sh

# è‡ªå®šä¹‰é…ç½®
BATCH=4 SEQLEN=2048 NHEADS=16 ./auto_profile.sh
```

**ç”Ÿæˆçš„æ–‡ä»¶**:
```
profile_results/
â”œâ”€â”€ SUMMARY.md                      # ğŸ“„ æ€»ç»“æŠ¥å‘Š (ä»è¿™é‡Œå¼€å§‹çœ‹!)
â”œâ”€â”€ baseline_benchmark.txt          # åŸºç¡€æ€§èƒ½æ•°æ®
â”œâ”€â”€ comparison_results.txt          # å®ç°å¯¹æ¯”ç»“æœ
â”œâ”€â”€ gpu_info.txt                    # GPUä¿¡æ¯
â”œâ”€â”€ flash_attn_nsys.nsys-rep       # ğŸ” Nsight SystemsæŠ¥å‘Š (GUIæ‰“å¼€)
â”œâ”€â”€ flash_attn_ncu_full.ncu-rep    # ğŸ” Nsight ComputeæŠ¥å‘Š (GUIæ‰“å¼€)
â”œâ”€â”€ nsys_stats.txt                  # Nsight Systemsç»Ÿè®¡
â”œâ”€â”€ ncu_memory_metrics.txt          # å†…å­˜å¸¦å®½åˆ†æ
â””â”€â”€ ncu_compute_metrics.txt         # è®¡ç®—å•å…ƒåˆ©ç”¨ç‡
```

## ğŸ” æ€§èƒ½åˆ†æå·¥å…·

### A. Nsight Systems (ç³»ç»Ÿçº§åˆ†æ)
**æŸ¥çœ‹æ•´ä½“æ€§èƒ½ã€kernelæ—¶é—´çº¿ã€æ‰¾ç“¶é¢ˆ**

```bash
# æ–¹æ³•1: ä½¿ç”¨auto_profile.sh (æ¨è)
./auto_profile.sh

# æ–¹æ³•2: æ‰‹åŠ¨è¿è¡Œ
nsys profile -o flash_profile --stats=true \
    python minimal_benchmark.py --no-grad
```

**å¦‚ä½•æŸ¥çœ‹ç»“æœ**:
1. ä¸‹è½½ `flash_profile.nsys-rep`
2. åœ¨[Nsight Systems GUI](https://developer.nvidia.com/nsight-systems)ä¸­æ‰“å¼€
3. æŸ¥çœ‹:
   - CUDA Kernelæ—¶é—´çº¿
   - GPUåˆ©ç”¨ç‡
   - å†…å­˜å¸¦å®½
   - æ‰¾å‡ºæœ€æ…¢çš„kernel

### B. Nsight Compute (Kernelçº§åˆ†æ)
**æ·±å…¥åˆ†æå•ä¸ªkernelã€ä¼˜åŒ–æŒ‡å¯¼**

```bash
# æ–¹æ³•1: ä½¿ç”¨auto_profile.sh (æ¨è)
./auto_profile.sh

# æ–¹æ³•2: æ‰‹åŠ¨è¿è¡Œ
ncu --set full -o flash_kernel \
    --kernel-name regex:"flash_fwd" \
    python minimal_benchmark.py --no-grad
```

**å¦‚ä½•æŸ¥çœ‹ç»“æœ**:
1. ä¸‹è½½ `flash_kernel.ncu-rep`
2. åœ¨[Nsight Compute GUI](https://developer.nvidia.com/nsight-compute)ä¸­æ‰“å¼€
3. æŸ¥çœ‹:
   - Memoryå¸¦å®½åˆ©ç”¨ç‡ (ç›®æ ‡: >80%)
   - SMæ•ˆç‡ (ç›®æ ‡: >90%)
   - Tensor Coreåˆ©ç”¨ç‡
   - Warp stallåŸå› 
   - ä¼˜åŒ–å»ºè®®

### C. å®æ—¶ç›‘æ§

**ç»ˆç«¯1 - è¿è¡Œbenchmark**:
```bash
python minimal_benchmark.py --repeats 1000
```

**ç»ˆç«¯2 - å®æ—¶ç›‘æ§GPU**:
```bash
# ä½¿ç”¨nvidia-smi
watch -n 0.5 nvidia-smi

# æˆ–ä½¿ç”¨nvtop (æ›´å¥½çœ‹)
nvtop
```

## ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡

### 1. TFLOPs/s (ååé‡)
- **A100 ç†è®ºå³°å€¼**: ~312 TFLOPs (FP16)
- **Flash Attention å…¸å‹**: 150-250 TFLOPs
- **æ›´é«˜ = æ›´å¥½**

### 2. æ‰§è¡Œæ—¶é—´
| Sequence Length | Batch Size | å…¸å‹æ—¶é—´ (A100) |
|----------------|------------|-----------------|
| 512            | 8          | ~1 ms           |
| 1024           | 4          | ~2 ms           |
| 2048           | 2          | ~8 ms           |
| 4096           | 1          | ~30 ms          |

### 3. å†…å­˜ä½¿ç”¨
Flash Attentionçš„ä¼˜åŠ¿:
- **æ ‡å‡†Attention**: O(NÂ²) å†…å­˜
- **Flash Attention**: O(N) å†…å­˜
- **é•¿åºåˆ—ä¸‹èŠ‚çœæ˜æ˜¾**

### 4. GPUåˆ©ç”¨ç‡
- **ç›®æ ‡**: >90%
- **æŸ¥çœ‹æ–¹æ³•**: `nvidia-smi` æˆ– Nsight Systems

## ğŸ¯ å¸¸è§æµ‹è¯•åœºæ™¯

### åœºæ™¯1: GPTæ¨¡å‹ (Decoder)
```bash
# GPT-2 Small
python minimal_benchmark.py --batch 8 --seqlen 1024 --nheads 12 --headdim 64 --causal

# GPT-3 Large
python minimal_benchmark.py --batch 2 --seqlen 2048 --nheads 96 --headdim 128 --causal
```

### åœºæ™¯2: BERTæ¨¡å‹ (Encoder)
```bash
# BERT-Base
python minimal_benchmark.py --batch 16 --seqlen 512 --nheads 12 --headdim 64

# BERT-Large
python minimal_benchmark.py --batch 8 --seqlen 512 --nheads 16 --headdim 64
```

### åœºæ™¯3: é•¿æ–‡æœ¬å¤„ç†
```bash
# 8K context
python minimal_benchmark.py --batch 1 --seqlen 8192 --nheads 8 --headdim 64

# 16K context
python minimal_benchmark.py --batch 1 --seqlen 16384 --nheads 8 --headdim 64
```

### åœºæ™¯4: æ‰¹é‡æ¨ç†
```bash
# é«˜batch size
python minimal_benchmark.py --batch 32 --seqlen 512 --nheads 8 --headdim 64 --no-grad
```

## ğŸ”§ æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥

è¿è¡Œæ­¤å‘½ä»¤æ£€æŸ¥ç³»ç»Ÿé…ç½®:
```bash
# GPUçŠ¶æ€
nvidia-smi

# GPUé¢‘ç‡ (åº”è¯¥åœ¨æœ€å¤§å€¼)
nvidia-smi -q -d CLOCK | grep "Graphics"

# è®¾ç½®æ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1

# è®¾ç½®æœ€å¤§é¢‘ç‡
sudo nvidia-smi -lgc 1410,1410  # A100çš„å€¼
```

## ğŸ“Š ç»“æœè§£è¯»

### å¥½çš„æ€§èƒ½è¡¨ç°
âœ… TFLOPs/s > 150 (A100, FP16)  
âœ… GPUåˆ©ç”¨ç‡ > 90%  
âœ… å†…å­˜å¸¦å®½åˆ©ç”¨ç‡ > 80%  
âœ… ç›¸æ¯”PyTorchåŠ é€Ÿ > 3x

### éœ€è¦ä¼˜åŒ–çš„æƒ…å†µ
âš ï¸ TFLOPs/s < 100  
âš ï¸ GPUåˆ©ç”¨ç‡ < 70%  
âš ï¸ é¢‘ç¹çš„CUDA OOMé”™è¯¯  
âš ï¸ åŠ é€Ÿæ¯” < 2x

### ä¼˜åŒ–å»ºè®®
1. **å¢åŠ batch size** (å¦‚æœå†…å­˜å…è®¸)
2. **ä½¿ç”¨FP16/BF16** è€ŒéFP32
3. **æ£€æŸ¥GPUé¢‘ç‡** æ˜¯å¦è¢«é™åˆ¶
4. **å…³é—­å…¶ä»–GPUè¿›ç¨‹**
5. **æ›´æ–°CUDA/PyTorch** åˆ°æœ€æ–°ç‰ˆæœ¬

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: ImportError: cannot import flash_attn
```bash
# é‡æ–°å®‰è£…
pip uninstall flash-attn -y
python setup.py install
```

### é—®é¢˜2: CUDA out of memory
```bash
# å‡å°batch sizeæˆ–sequence length
python minimal_benchmark.py --batch 1 --seqlen 1024
```

### é—®é¢˜3: ç¼–è¯‘å¤±è´¥
```bash
# é™åˆ¶å¹¶è¡Œç¼–è¯‘æ•°
MAX_JOBS=4 python setup.py install

# ç¡®ä¿ninjaå·²å®‰è£…
pip install ninja
```

### é—®é¢˜4: æ€§èƒ½å¼‚å¸¸ä½
```bash
# æ£€æŸ¥GPUæ¨¡å¼
nvidia-smi -q -d PERFORMANCE

# æ£€æŸ¥æ˜¯å¦æœ‰åå°è¿›ç¨‹
nvidia-smi

# é‡å¯GPU
sudo nvidia-smi -r
```

### é—®é¢˜5: nsys/ncu å‘½ä»¤æ‰¾ä¸åˆ°
```bash
# éœ€è¦å®‰è£…å®Œæ•´CUDA Toolkit (ä¸åªæ˜¯runtime)
# ä¸‹è½½åœ°å€: https://developer.nvidia.com/cuda-downloads

# æ£€æŸ¥å®‰è£…
which nsys
which ncu

# æ·»åŠ åˆ°PATH (å¦‚æœå·²å®‰è£…ä½†æ‰¾ä¸åˆ°)
export PATH=/usr/local/cuda/bin:$PATH
```

## ğŸ“š è¿›é˜¶ä½¿ç”¨

### æ‰¹é‡æµ‹è¯•
```bash
#!/bin/bash
for seqlen in 512 1024 2048 4096; do
    echo "Testing seqlen=$seqlen"
    python minimal_benchmark.py --seqlen $seqlen | grep "TFLOPs/s (avg)"
done
```

### è‡ªåŠ¨åŒ–å›å½’æµ‹è¯•
```bash
# ä¿å­˜baseline
python minimal_benchmark.py > baseline.txt

# ä¿®æ”¹ä»£ç åæµ‹è¯•
python minimal_benchmark.py > current.txt

# å¯¹æ¯”
diff baseline.txt current.txt
```

### å¯¼å‡ºç»“æœ
```python
# åœ¨minimal_benchmark.pyä¸­æ·»åŠ 
import json
results = {
    'time_ms': avg_time,
    'tflops': tflops_avg,
    'memory_gb': memory_allocated
}
with open('results.json', 'w') as f:
    json.dump(results, f)
```

## ğŸ“ å­¦ä¹ èµ„æº

### æ ¸å¿ƒæ–‡ä»¶
- `csrc/flash_attn/src/flash_fwd_kernel.h` - A100 forward kernelå®ç°
- `csrc/flash_attn/src/flash_bwd_kernel.h` - A100 backward kernelå®ç°  
- `hopper/flash_fwd_kernel_sm90.h` - H100 (Hopper) ä¼˜åŒ–ç‰ˆæœ¬

### è®ºæ–‡
- [FlashAttention v1](https://arxiv.org/abs/2205.14135)
- [FlashAttention v2](https://tridao.me/publications/flash2/flash2.pdf)
- [FlashAttention v3](https://tridao.me/publications/flash3/flash3.pdf)

### Cutlasså­¦ä¹ 
- [Cutlass GitHub](https://github.com/NVIDIA/cutlass)
- [Cutlass Examples](https://github.com/NVIDIA/cutlass/tree/main/examples)

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä»å°é…ç½®å¼€å§‹**: å…ˆæµ‹è¯•å°çš„batch/seqlenï¼Œç¡®ä¿æ­£ç¡®
2. **ä½¿ç”¨--no-grad**: åšprofilingæ—¶åªæµ‹forward pass
3. **é‡å¤å¤šæ¬¡**: è‡³å°‘30æ¬¡ä»¥ä¸Šè·å¾—ç¨³å®šç»“æœ
4. **è®°å½•ç¯å¢ƒ**: GPUå‹å·ã€CUDAç‰ˆæœ¬ã€PyTorchç‰ˆæœ¬
5. **å¯¹æ¯”baseline**: æ€»æ˜¯å’ŒPyTorchæ ‡å‡†å®ç°å¯¹æ¯”

## ğŸ“ è·å–å¸®åŠ©

- **GitHub Issues**: https://github.com/Dao-AILab/flash-attention/issues
- **è®ºæ–‡ä½œè€…**: Tri Dao
- **è¯¦ç»†æ–‡æ¡£**: `BENCHMARK_GUIDE.md`

---

**ç¥ä½ Benchmarké¡ºåˆ©ï¼** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œå¯ä»¥æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£: `BENCHMARK_GUIDE.md`

