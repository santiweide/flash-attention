# Flash Attention æœ€å°å®ç° - æ¶æ„è¯´æ˜

## ğŸ“ æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Flash Attention                         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Q [B,H,S,D] â”‚  â”‚  K [B,H,S,D] â”‚  â”‚  V [B,H,S,D] â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                 â”‚                 â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚              â”‚  Flash Attention Kernelâ”‚                     â”‚
â”‚              â”‚                        â”‚                     â”‚
â”‚              â”‚  - Tiling              â”‚                     â”‚
â”‚              â”‚  - Online Softmax      â”‚                     â”‚
â”‚              â”‚  - Fused Operations    â”‚                     â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                           â”‚                                 â”‚
â”‚                           â–¼                                 â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚                  â”‚  O [B,H,S,D] â”‚                           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ ç®—æ³•æµç¨‹

### é«˜å±‚æµç¨‹

```python
for each Q_block in Q:  # å¤–å¾ªç¯ï¼šéå†Qçš„blocks
    initialize m = -inf, l = 0, O = 0
    
    for each KV_block in K,V:  # å†…å¾ªç¯ï¼šéå†K,Vçš„blocks
        # 1. è®¡ç®—attention scores
        S = Q_block @ K_block^T * scale
        
        # 2. åœ¨çº¿æ›´æ–°softmaxç»Ÿè®¡é‡
        m_old, l_old = m, l
        m = max(m_old, rowmax(S))
        
        # 3. è®¡ç®—attention weights
        P = exp(S - m)
        l = exp(m_old - m) * l_old + rowsum(P)
        
        # 4. æ›´æ–°è¾“å‡ºï¼ˆå…³é”®ï¼šéœ€è¦ä¿®æ­£ä¹‹å‰çš„ç´¯åŠ å€¼ï¼‰
        correction = exp(m_old - m)
        O = correction * O + P @ V_block / l
    
    write O to global memory
```

### è¯¦ç»†æ‰§è¡Œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: åŠ è½½Q blockåˆ°å…±äº«å†…å­˜                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
â”‚  â”‚ Q_block  â”‚  [kBlockM, kHeadDim]                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: å¾ªç¯å¤„ç†æ¯ä¸ªK,V block                      â”‚
â”‚                                                    â”‚
â”‚  for each KV_block:                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚  â”‚ K_block  â”‚  â”‚ V_block  â”‚                       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                    â”‚
â”‚  a) è®¡ç®— S = Q_block @ K_block^T                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚     â”‚  S [M x N]      â”‚                           â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                    â”‚
â”‚  b) åœ¨çº¿æ›´æ–°softmax                                 â”‚
â”‚     - æ›´æ–° m (max)                                 â”‚
â”‚     - æ›´æ–° l (sum of exp)                          â”‚
â”‚     - è®¡ç®— P = exp(S - m)                          â”‚
â”‚                                                    â”‚
â”‚  c) ç´¯åŠ è¾“å‡º                                        â”‚
â”‚     O = correction * O + P @ V_block               â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: å½’ä¸€åŒ–å¹¶å†™å›å…¨å±€å†…å­˜                        â”‚
â”‚  O = O / l                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ’¾ å†…å­˜å±‚æ¬¡

### å…¨å±€å†…å­˜ (HBM)
- **Q, K, V, O**: è¾“å…¥è¾“å‡ºtensors
- **å¤§å°**: `batch * num_heads * seq_len * head_dim`
- **å¸¦å®½**: ~2 TB/s (A100)
- **å»¶è¿Ÿ**: ~100ns

### å…±äº«å†…å­˜ (SRAM)
- **Q_block**: `[kBlockM, kHeadDim]` = `[64, 64]` = 8KB (FP16)
- **K_block**: `[kBlockN, kHeadDim]` = `[64, 64]` = 8KB (FP16)
- **V_block**: `[kBlockN, kHeadDim]` = `[64, 64]` = 8KB (FP16)
- **S**: `[kBlockM, kBlockN]` = `[64, 64]` = 16KB (FP32)
- **P**: `[kBlockM, kBlockN]` = `[64, 64]` = 16KB (FP32)
- **æ€»è®¡**: ~56KB
- **å¸¦å®½**: ~19 TB/s (A100)
- **å»¶è¿Ÿ**: ~5ns

### å¯„å­˜å™¨
- **çº¿ç¨‹å±€éƒ¨å˜é‡**
- **ç´¯åŠ å™¨**
- **å¸¦å®½**: æœ€å¿«
- **å»¶è¿Ÿ**: 1 cycle

## ğŸ§® è®¡ç®—åˆ†æ

### FLOPsè®¡ç®—

å¯¹äºä¸€ä¸ªattentionæ“ä½œ `O = softmax(Q @ K^T) @ V`:

1. **Q @ K^T**: `2 * M * N * D` FLOPs
2. **Softmax**: `5 * M * N` FLOPs (exp, max, sum, div)
3. **P @ V**: `2 * M * N * D` FLOPs

**æ€»è®¡**: `4 * M * N * D + 5 * M * N â‰ˆ 4 * M * N * D` FLOPs

å¯¹äº `seq_len=S, head_dim=D`:
- **Total FLOPs** = `4 * S^2 * D * batch * num_heads`

### å†…å­˜è®¿é—®åˆ†æ

#### æ ‡å‡†å®ç°
```
Q: S * D      (è¯»)
K: S * D      (è¯»)
S: S * S      (å†™+è¯»)  â† ç“¶é¢ˆï¼
V: S * D      (è¯»)
O: S * D      (å†™)
---
Total: 2*S*D + 2*S*S + S*D = S*(3*D + 2*S)
```

å½“ S >> D æ—¶ï¼Œä¸»å¯¼é¡¹æ˜¯ `2*S*S`ï¼Œå¤æ‚åº¦ **O(SÂ²)**

#### Flash Attention
```
Q: S * D      (è¯»)
K: S * D      (è¯»)
V: S * D      (è¯»)
O: S * D      (å†™)
---
Total: 4*S*D
```

å¤æ‚åº¦ **O(S)**ï¼ŒèŠ‚çœäº† `S*S` çš„ä¸­é—´å­˜å‚¨ï¼

### ç†è®ºåŠ é€Ÿæ¯”

```
å†…å­˜è®¿é—®æ—¶é—´ = æ•°æ®é‡ / å¸¦å®½

æ ‡å‡†å®ç°æ—¶é—´ â‰ˆ S*(3*D + 2*S) / BW_HBM
Flash Attnæ—¶é—´ â‰ˆ 4*S*D / BW_SRAM + è®¡ç®—æ—¶é—´

å½“ S >> D æ—¶:
åŠ é€Ÿæ¯” â‰ˆ (2*S*S / BW_HBM) / (4*S*D / BW_SRAM)
      = (S * BW_SRAM) / (2*D * BW_HBM)
      â‰ˆ (S * 19000) / (2*D * 2000)  # A100æ•°æ®
      = 4.75 * S / D
```

å¯¹äº `S=1024, D=64`:
- ç†è®ºåŠ é€Ÿæ¯” â‰ˆ **76x** (ä»…è€ƒè™‘å†…å­˜)

å®é™…åŠ é€Ÿæ¯”çº¦ **3-8x**ï¼Œå› ä¸ºè¿˜æœ‰è®¡ç®—æ—¶é—´ã€‚

## ğŸ¯ å…³é”®ä¼˜åŒ–æŠ€æœ¯

### 1. Tiling (åˆ†å—)

**ç›®çš„**: å°†å¤§çŸ©é˜µåˆ†æˆå°å—ï¼Œæ¯å—fitè¿›å…±äº«å†…å­˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Q [seq_len, head_dim]         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Q0 â”‚ Q1 â”‚ Q2 â”‚  [BlockM,D]  â”‚
â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤              â”‚
â”‚  â”‚ Q3 â”‚ Q4 â”‚ Q5 â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  K [seq_len, head_dim]         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ K0 â”‚ K1 â”‚  [BlockN, D]      â”‚
â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤                   â”‚
â”‚  â”‚ K2 â”‚ K3 â”‚                   â”‚
â”‚  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤                   â”‚
â”‚  â”‚ K4 â”‚ K5 â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å—å¤§å°é€‰æ‹©**:
- å¤ªå°ï¼šå¯åŠ¨å¼€é”€å¤§
- å¤ªå¤§ï¼šå…±äº«å†…å­˜ä¸å¤Ÿ
- å…¸å‹å€¼ï¼š64x64, 128x128

### 2. åœ¨çº¿Softmax

**æ ‡å‡†ä¸¤æ¬¡éå†**:
```python
# Pass 1: æ‰¾max
m = max(x)

# Pass 2: è®¡ç®—expå’Œsum
s = sum(exp(x - m))

# Pass 3: å½’ä¸€åŒ–
y = exp(x - m) / s
```

**åœ¨çº¿å•æ¬¡éå†** (Flash Attentionæ ¸å¿ƒ):
```python
m_old, l_old = current_max, current_sum

# å¤„ç†æ–°æ•°æ®
m_new = max(m_old, max(x_new))

# æ›´æ–°ç»Ÿè®¡é‡
correction = exp(m_old - m_new)
l_new = correction * l_old + sum(exp(x_new - m_new))

# ä¿®æ­£ä¹‹å‰çš„è¾“å‡º
y_old = y_old * correction * (l_old / l_new)
y_new = exp(x_new - m_new) / l_new

# åˆå¹¶
y = y_old + y_new
```

### 3. Kernelèåˆ

**æ ‡å‡†å®ç°** (å¤šä¸ªkernel):
```
kernel1: S = Q @ K^T
kernel2: S_max = rowmax(S)
kernel3: S_exp = exp(S - S_max)
kernel4: S_sum = rowsum(S_exp)
kernel5: P = S_exp / S_sum
kernel6: O = P @ V
```
æ¯ä¸ªkerneléƒ½éœ€è¦è¯»å†™å…¨å±€å†…å­˜ â†’ **6x å…¨å±€å†…å­˜è®¿é—®**

**Flash Attention** (å•ä¸ªkernel):
```
kernel_fused: 
    - è®¡ç®— S
    - è®¡ç®— softmax (åœ¨çº¿)
    - è®¡ç®— O
    - å…¨éƒ¨åœ¨å…±äº«å†…å­˜ä¸­
```
åªåœ¨å¼€å§‹å’Œç»“æŸæ—¶è®¿é—®å…¨å±€å†…å­˜ â†’ **1x å…¨å±€å†…å­˜è®¿é—®**

### 4. å…±äº«å†…å­˜ä¼˜åŒ–

**Bank Conflicté¿å…**:
```cpp
// Bad: æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä¸ªbank
shared_mem[threadIdx.x * N]

// Good: ä½¿ç”¨paddingæˆ–swizzling
shared_mem[threadIdx.x * (N + 1)]
```

**Swizzling**: æ‰“ä¹±å†…å­˜å¸ƒå±€ä»¥é¿å…bank conflict
```
ä¸ä½¿ç”¨swizzle:     ä½¿ç”¨swizzle:
[0][1][2][3]      [0][2][1][3]
[4][5][6][7]  â†’   [4][6][5][7]
[8][9][A][B]      [8][A][9][B]
```

## ğŸ“Š æ€§èƒ½ç‰¹å¾

### è®¡ç®—vså†…å­˜bound

```
Compute Intensity = FLOPs / Bytes

æ ‡å‡†Attention:
  FLOPs = 4*S^2*D
  Bytes = S*(3*D + 2*S)
  Intensity â‰ˆ (4*S*D) / (2*S) = 2*D
  â†’ Memory bound (å¯¹äºD=64-128)

Flash Attention:
  FLOPs = 4*S^2*D (same)
  Bytes = 4*S*D
  Intensity â‰ˆ S
  â†’ Compute bound (å½“Sè¶³å¤Ÿå¤§)
```

### å¯æ‰©å±•æ€§

| Seq Length | æ ‡å‡†Attnå†…å­˜ | Flash Attnå†…å­˜ | å†…å­˜èŠ‚çœ |
|-----------|-------------|---------------|---------|
| 512 | 512KB | 128KB | 4x |
| 1024 | 2MB | 256KB | 8x |
| 2048 | 8MB | 512KB | 16x |
| 4096 | 32MB | 1MB | 32x |
| 8192 | 128MB | 2MB | 64x |

**ç»“è®º**: åºåˆ—è¶Šé•¿ï¼ŒFlash Attentionä¼˜åŠ¿è¶Šæ˜æ˜¾ï¼

## ğŸ”¬ ä¸å®Œæ•´å®ç°çš„å·®å¼‚

| ç‰¹æ€§ | æœ€å°å®ç° | å®Œæ•´å®ç° | å½±å“ |
|-----|---------|---------|------|
| å†…å­˜å¸ƒå±€ | ç®€å• | Swizzled | æ€§èƒ½ |
| GEMM | ç®€å•å¾ªç¯ | Cutlassä¼˜åŒ– | æ€§èƒ½ |
| å¯„å­˜å™¨ä½¿ç”¨ | æœ€å° | ä¼˜åŒ– | æ€§èƒ½ |
| Bank conflict | æœªä¼˜åŒ– | ä¼˜åŒ– | æ€§èƒ½ |
| Warpçº§ä¼˜åŒ– | æ—  | æœ‰ | æ€§èƒ½ |
| Causal mask | æ—  | æœ‰ | åŠŸèƒ½ |
| Dropout | æ—  | æœ‰ | åŠŸèƒ½ |
| å‘åä¼ æ’­ | æ—  | æœ‰ | åŠŸèƒ½ |

**æ€§èƒ½å·®è·**: æœ€å°å®ç°çº¦ä¸ºå®Œæ•´å®ç°çš„ **50-70%**

## ğŸ“ å­¦ä¹ å»ºè®®

### ç¬¬1é˜¶æ®µ: ç†è§£ç®—æ³•
1. é˜…è¯»Flash Attentionè®ºæ–‡
2. ç†è§£åœ¨çº¿softmaxçš„æ•°å­¦åŸç†
3. æ‰‹åŠ¨è®¡ç®—ä¸€ä¸ªå°ä¾‹å­ (2x2çŸ©é˜µ)

### ç¬¬2é˜¶æ®µ: ç†è§£å®ç°
1. é˜…è¯» `flash_attn_minimal.cu`
2. ç†è§£å†…å­˜å¸ƒå±€å’Œæ•°æ®æµ
3. å¯¹æ¯”å‚è€ƒå®ç°ï¼Œæ‰¾å‡ºå·®å¼‚

### ç¬¬3é˜¶æ®µ: å®éªŒ
1. ä¿®æ”¹å—å¤§å°ï¼Œè§‚å¯Ÿæ€§èƒ½å˜åŒ–
2. æ·»åŠ ç®€å•çš„profiling
3. å°è¯•æ·»åŠ causal mask

### ç¬¬4é˜¶æ®µ: ä¼˜åŒ–
1. å­¦ä¹ Cutlassçš„GEMMå®ç°
2. å®ç°swizzling
3. ä¼˜åŒ–bank conflict
4. ä½¿ç”¨Nsight Computeåˆ†æ

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [FlashAttention: Fast and Memory-Efficient Exact Attention](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism](https://tridao.me/publications/flash2/flash2.pdf)

### ä»£ç 
- [Flash Attention Official](https://github.com/Dao-AILab/flash-attention)
- [CUTLASS](https://github.com/NVIDIA/cutlass)
- [CUTLASS CUTE](https://github.com/NVIDIA/cutlass/tree/main/media/docs/cute)

### æ•™ç¨‹
- [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [Shared Memory Best Practices](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/)

---

å¸Œæœ›è¿™ä»½æ¶æ„è¯´æ˜å¸®åŠ©ä½ æ·±å…¥ç†è§£Flash Attentionçš„å®ç°ï¼ğŸš€

